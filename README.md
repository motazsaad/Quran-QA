# Quran-QA
Quran QA


# Trained Models 

## arap_qa_bert_large_v2
https://drive.google.com/drive/folders/18MJRJIizHk2eD_a8drvGUOMpoMSss1PF?usp=share_link

## wissamantoun
https://drive.google.com/drive/folders/1L67pKnljW2IHkxTUFuDJe6bK_actJQHi?usp=share_link

## salti
https://drive.google.com/drive/folders/1wtA7WQjHgU7T7uE4eIbgf5Wfzej409pg?usp=share_link


# Citation 
Please cite 
```
@inproceedings{ahmed-etal-2022-qqateam,
    title = "{QQAT}eam at Qur{'}an {QA} 2022: Fine-Tunning {A}rabic {QA} Models for Qur{'}an {QA} Task",
    author = "Ahmed, Basem  and
      Saad, Motaz  and
      Refaee, Eshrag A.",
    booktitle = "Proceedinsg of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Qur'an QA and Fine-Grained Hate Speech Detection",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.osact-1.16",
    pages = "130--135",
    abstract = "The problem of auto-extraction of reliable answers from a reference text like a constitution or holy book is a real challenge for the natural languages research community. Qur{\'a}n is the holy book of Islam and the primary source of legislation for millions of Muslims around the world, which can trigger the curiosity of non-Muslims to find answers about various topics from the Qur{\'a}n. Previous work on Question Answering (Q{\&}A) from Qur{\'a}n is scarce and lacks the benchmark of previously developed systems on a testbed to allow meaningful comparison and identify developments and challenges. This work presents an empirical investigation of our participation in the Qur{\'a}n QA shared task (2022) that utilizes a benchmark dataset of 1,093 tuples of question-Qur{\'a}n passage pairs. The dataset comprises Qur{\'a}n verses, questions and several ranked possible answers. This paper describes the approach we follow with our participation in the shared task and summarises our main findings. Our system attained the best score at 0.63 pRR and 0.59 F1 on the development set and 0.56 pRR and 0.51 F1 on the test set. The best results of the Exact Match (EM) score at 0.34 indicate the difficulty of the task and the need for more future work to tackle this challenging task.",
}
```
